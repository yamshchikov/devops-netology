# Homework 3.8

1.  Соединения в состоянии TIME-WAIT ждут, пока пройдет 2MSL перед закрытием.

2.  Я создал три виртуальные машины в одной локальной сети. Vagrant, (там поднят виртуальный IP с помощью keepalived),
    test1 и test2 (на них запущен nginx). На хосте vagrant запущен keepalived с таким конфигом:
   
    ```
    vrrp_instance VI_1 {
        state MASTER
        interface eth0
        virtual_router_id 50
        priority 100
        advert_int 1
        virtual_ipaddress {
            192.168.56.200/32 dev eth0
        }
    }
    ```
   
    Также, keepalived крутится на хосте test1. Решил не создавать четвертый хост лишь для этого. Там конфиг такой:
   
    ```
    vrrp_instance VI_1 {
        state BACKUP
        interface eth0
        virtual_router_id 50
        priority 50
        advert_int 1
        virtual_ipaddress {
            192.168.56.200/32 dev eth0
        }
    }
    ```
    
    Тестил с основной (физической) машины, просто вводя IP адрес в строку браузера.   

    При ручной остановке демона keepalived на хосте vagrant, VIP пропадает и автоматически поднимается на машине test1.  
    При выключении, перезагрузке и т.д. - тоже. Потом автоматом возвращается на место после восстановления vagrant.  
    Заглушка nginx отображается в браузере во всех случаях.

3.  Я никак не могу до конца понять сути задания, поэтому попробую размышлять на ходу прямо здесь.
    
    У нас есть три хоста, на каждом из которых работает балансировщик траффика.
    Каждый из хостов может принимать максимум 1 Гбит/c. А общий объем трафика, который приходит на три хоста - 1.5
    Гбит/с. Как к нам приходит этот поток данных? 
    
    Если всё идет одним потоком, то нам нужен балансировщик более высокого уровня на хосте, который имеет подходящий
    линк (а лучше 2 Гбит/с). Один VIP на этом хосте и ещё три на нижних - всего 4. Если любой из трех нижних выйдет из 
    строя, то нагрузка распределится на два других и всё будет в порядке. Правда, первый балансировщик в таком случае 
    является единой точкой отказа. И непонятно, зачем нужно два уровня L4 балансировки. Ведь со всем этим прекрасно 
    справится один хост, имеющий достаточную пропускную способность. Он так и так является точкой отказа. Зачем нам
    ещё 3 балансировщика?
    
    Если же мы изначально имеем три равных потока данных по 500 Мбит/с и нам не важно, что там выше и откуда эти данные
    идут, то нам надо 3 VIP по одному на каждый хост. Каждый из них спокойно принимает свои 500 Мбит/с и когда один из
    них выходит из строя, то этот IP адрес нужно автоматом поднять на одном из оставшихся хостов с помощью keepalived.
    Тогда ресурсов будет впритык - полный 1 Гбит/с, так что деградация возможна. Но чтобы 
    распределить эти 500 равномерно, нам нужен ещё один балансировщик на отдельном хосте. Но ведь он тогда может просто 
    забрать на себя обработку полностью и ничего не делить.
    
    Может имелось в виду, что у нас есть три бэкенда с линком 1 Гбит/с на каждом? И нужно просто отбалансировать
    входяшие 1.5 Гбит/с? Но тогда просто один балансировщик с одним VIP и равномерным распределением вполне справится. 
    Если один из бэкендов выйдет из строя, то на оставшиеся два придется по 750 Мбит/с и они справятся.
    
    Прошу прощения, что предоставляю ответ в таком виде. Но я вроде разобрал все варианты, как можно трактовать задание,
    а вы видите ход моих мыслей и вам будет проще меня поправить, если что-то не так. Всё-таки мне кажется, что я не 
    верно понял задание.